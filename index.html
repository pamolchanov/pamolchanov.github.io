<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Pavlo Molchanov</title>
    <meta name="description" content="Pavlo Molchanov â€” Research Director at NVIDIA Research. LLM/VLM efficiency, compression, NAS, adaptive inference." />
    <link rel="icon" type="image/png" href="assets/images/thumbnail.png" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet" />
    <link rel="stylesheet" href="assets/styles.css" />
  </head>
  <body>
    <nav class="navbar">
      <div class="nav-container">
        <a href="#home" class="nav-brand">Pavlo Molchanov</a>
        <ul class="nav-menu">
          <li class="nav-item">
            <a href="#home" class="nav-link active">About</a>
          </li>
          <li class="nav-item">
            <a href="#publications" class="nav-link">Publications</a>
          </li>
          <li class="nav-item">
            <a href="#news" class="nav-link">News</a>
          </li>
          <li class="nav-item">
            <a href="#bio" class="nav-link">Bio</a>
          </li>
        </ul>
        <div class="hamburger">
          <span class="bar"></span>
          <span class="bar"></span>
          <span class="bar"></span>
        </div>
      </div>
    </nav>
    
    <div class="container">
      <header class="profile-header">
        <div class="profile-layout">
          <div class="profile-info">
            <h1 class="name">Pavlo Molchanov</h1>
            <p class="bio">
              I'm a Research Director at NVIDIA leading DLER team (<a href="https://nv-dler.github.io">webpage</a>) at NVIDIA Research. 
              My research focuses on efficiency in LLMs and multi-modal models: 
              compression, NAS-like acceleration, novel architectures, and adaptive/conditional inference.
            </p>
            <p class="bio">
                

            </p>
            <div class="links">
              <a href="https://scholar.google.com/citations?user=J9PoyoIAAAAJ&hl=en&oi=ao" target="_blank">Google Scholar</a> / 
              <a href="https://x.com/PavloMolchanov" target="_blank">X</a> / 
              <a href="https://www.linkedin.com/in/pavlo-molchanov-08738a63/" target="_blank">LinkedIn</a><br />
              <a href="mailto:pmolchanov@nvidia.com">pmolchanov [at] nvidia.com</a> /
              <a href="mailto:pamolchanov@gmail.com">pamolchanov [at] gmail.com</a> 
            </div>
          </div>
          <div class="profile-image">
            <img src="assets/profile.png" alt="Pavlo Molchanov" />
          </div>
        </div>
      </header>


      <section id="news" class="section">
        <h2>News</h2>
        <ul id="newsList" class="news-list"></ul>
      </section>

      <section id="publications" class="section">
        <h2>Projects and publications (<a href="https://scholar.google.com/citations?user=J9PoyoIAAAAJ&hl=en&oi=ao" target="_blank">full list</a>)</h2>

        <div class="pub-categories">
          <div class="pub-category">
            <h3>Foundation Models</h3>
            <div id="foundationModels" class="pub-subsection"></div>
          </div>
          
          <div class="pub-category">
            <h3>Post-training Optimization</h3>
            <div id="postTraining" class="pub-subsection"></div>
          </div>
          
          <div class="pub-category">
            <h3>Multi-Modal Models</h3>
            <div id="multiModal" class="pub-subsection"></div>
          </div>
          
          <div class="pub-category">
            <h3>Vision Encoders</h3>
            <div id="visionEncoders" class="pub-subsection"></div>
          </div>
        </div>
      </section>



      <section class="service-section">
        <h2>Professional Service</h2>
        <p>
          I am happy to give keynotes and invited talks, contact me.
        </p>
      </section>

      <section class="awards-section">
        <h2>Awards & Recognition</h2>
        <ul class="awards-list">
          <li><strong>2014:</strong> Best Paper Award, EuRAD Conference</li>
          <li><strong>2014:</strong> Young Researcher Award, EuRAD Conference</li>
          <li><strong>2012-2014:</strong> Nokia Foundation Scholarship</li>
          <li><strong>2011-2014:</strong> GETA Graduate School Grant</li>
        </ul>
      </section>

      <section id="bio" class="section">
        <h2>Bio</h2>
        <div class="bio-content">
          <p>
            Pavlo Molchanov obtained a PhD from Tampere University of Technology, Finland, in 2014, in the field of RADAR signal processing. 
            During his studies, he received the Nokia Foundation Scholarship, GETA Graduate School grant, Best Paper Award, and Young Researcher Award at EuRAD.
          </p>
          <p>
            Since 2015, he has been working in NVIDIA Research where he is now a Research Director leading a deep learning efficiency team. With the focus on LLMs and multi-modal models, he has been working on compression, NAS-like acceleration, novel architectures, and adaptive/conditional inference.
          </p>
          <p>
            His past research has led to several NVIDIA product integrations: hand, body, and facial 
            keypoint estimation and recognition in <a href="https://www.nvidia.com/en-us/self-driving-cars/drive-ix/" target="_blank">DriveIX</a>, <a href="https://www.nvidia.com/en-us/geforce/broadcasting/" target="_blank">Broadcast</a>, <a href="https://www.nvidia.com/en-us/omniverse/" target="_blank">Omniverse</a>, <a href="https://developer.nvidia.com/maxine" target="_blank">Maxine</a>; efficient 
            vision backbones in <a href="https://developer.nvidia.com/tao-toolkit" target="_blank">TAO</a>, developed compression techniques in <a href="https://developer.nvidia.com/tao-toolkit" target="_blank">TAO</a>, NVIDIA AV, <a href="https://developer.nvidia.com/tensorrt" target="_blank">TRT Model 
            Optimization</a>; and small in-game LLMs called <a href="https://developer.nvidia.com/blog/how-to-prune-and-distill-llama-3-1-8b-to-an-nvidia-llama-3-1-minitron-4b-model/" target="_blank">Minitron</a>. More recently, he has been working on design and compression of <a href="https://huggingface.co/collections/nvidia/nvidia-nemotron-689f6d6e6ead8e77dd641615" target="_blank">NVIDIA Nemotron</a> models.
          </p>
        </div>
      </section>
    </div>

    <script src="assets/site.js" defer></script>
  </body>
  </html>


